{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szn7YHgz4Q7h"
      },
      "source": [
        "**Задание**: LLM классификация отзывов\n",
        "\n",
        "**Метрика**: Weighted F1\n",
        "\n",
        "**Time**: 5 секунд"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9faHL7sx6hOx"
      },
      "source": [
        "# 1. Подготовка данных: text preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLT5DYBr8aPe"
      },
      "source": [
        "Анализируем train тексты"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qDJXzrXX3ymX"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wzqUHZDz6scG"
      },
      "outputs": [],
      "source": [
        "data_train = pd.read_csv('train.csv')['text'].tolist()\n",
        "data_train[:10]  # пример нескольких первых отзывов"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VayeI3yBbZe"
      },
      "source": [
        "Очистка и обработка текстов"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HE13r9hL6yHu"
      },
      "outputs": [],
      "source": [
        "# приводим к нижнему регистру\n",
        "data_train = [i.lower() for i in data_train]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2nFC3lp-CwKg"
      },
      "outputs": [],
      "source": [
        "# удалим стоп-слова, HTML-тэги, цифры и знаки препинания\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "\n",
        "nltk.download(\"stopwords\")\n",
        "stop_words = set(stopwords.words(\"russian\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ej3eeIS-CxoX"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    # убираем HTML тэги\n",
        "    text = BeautifulSoup(text, \"lxml\").text\n",
        "    # убираем цифры и спецсимволы, оставляем только буквы и пробелы\n",
        "    text = re.sub(r\"[^а-яА-Яa-zA-Z\\s]\", \" \", text)\n",
        "    # убираем лишние пробелы\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    # удаляем стоп-слова\n",
        "    tokens = text.split()\n",
        "    tokens = [w for w in tokens if w not in stop_words]\n",
        "    text = ' '.join(tokens)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCkFjxJrEPJG"
      },
      "outputs": [],
      "source": [
        "data_train = [clean_text(i) for i in data_train]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJsuASXbEXk-"
      },
      "outputs": [],
      "source": [
        "data_train[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Датасет небольшой, здесь сделай аугментацию данных для получения большего количества обучающих примеров - через Back Translation"
      ],
      "metadata": {
        "id": "zx-BWeG6FTrE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Back translation (ru - eng - ru)"
      ],
      "metadata": {
        "id": "9Eg_qGFWIY2E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "LNWizPXtFgFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Может потребовать установки моделей\n",
        "translator_en_ru = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-ru\")\n",
        "translator_ru_en = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-ru-en\")\n",
        "\n",
        "def back_translation(text):\n",
        "    # Русский -> Английский -> Русский\n",
        "    en_text = translator_ru_en(text)[0]['translation_text']\n",
        "    ru_text = translator_en_ru(en_text)[0]['translation_text']\n",
        "    return ru_text"
      ],
      "metadata": {
        "id": "PUsJOIJAGrVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train_ru_eng = []"
      ],
      "metadata": {
        "id": "ZPvbzc65Iixy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in data_train:\n",
        "  data_train_ru_eng.append(back_translation(i))"
      ],
      "metadata": {
        "id": "h15O8GzGIopl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train[:5]"
      ],
      "metadata": {
        "id": "Ck5tBvOwLTZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train_ru_eng[:5]"
      ],
      "metadata": {
        "id": "SU9BMuCPLLxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train_ru_eng = [clean_text(i) for i in data_train_ru_eng]"
      ],
      "metadata": {
        "id": "JsALVR3YLaf2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(data_train_ru_eng)"
      ],
      "metadata": {
        "id": "Jvn8PO87LhS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in data_train_ru_eng:\n",
        "  data_train.append(i)"
      ],
      "metadata": {
        "id": "hrHNovs_M9yg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(data_train)"
      ],
      "metadata": {
        "id": "pCIQOlH1NMC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUkGb3bUEwF1"
      },
      "source": [
        "Проведём **лемматизацию**: все слова приведём к начальной форме"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "AoG6yPSoFaef"
      },
      "outputs": [],
      "source": [
        "!pip install natasha"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EEExXPopEbw1"
      },
      "outputs": [],
      "source": [
        "from natasha import Doc, Segmenter, MorphVocab, NewsEmbedding, NewsMorphTagger\n",
        "\n",
        "segmenter = Segmenter()\n",
        "morph_vocab = MorphVocab()\n",
        "emb = NewsEmbedding()\n",
        "morph_tagger = NewsMorphTagger(emb)\n",
        "\n",
        "def lemmatization(text):\n",
        "    doc = Doc(text)\n",
        "    doc.segment(segmenter)\n",
        "    doc.tag_morph(morph_tagger)\n",
        "    for token in doc.tokens:\n",
        "        token.lemmatize(morph_vocab)\n",
        "    return \" \".join([t.lemma for t in doc.tokens])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pkQAmX2qFUwv"
      },
      "outputs": [],
      "source": [
        "data_train = [lemmatization(i) for i in data_train]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_train = [clean_text(text) for text in data_train]"
      ],
      "metadata": {
        "id": "i0iRx12cNsRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6ITvP50G_-K"
      },
      "source": [
        "1. Построим гистограмму длин отзывов по количеству слов"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-c6y0yxHGI72"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2R43FWsHcwx"
      },
      "outputs": [],
      "source": [
        "# строим гистограмму по количеству слов в отзыве\n",
        "plt.hist([len(i.split()) for i in data_train], bins=50)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9T80lWDH0QR"
      },
      "outputs": [],
      "source": [
        "# посчитаем процент отзывов с разным количеством слов\n",
        "small = len([i for i in data_train if len(i.split()) <= 2])*100 / len(data_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X4HgGwKJKRIR"
      },
      "outputs": [],
      "source": [
        "medium = len([i for i in data_train if len(i.split()) < 20 and len(i.split()) > 2])*100 / len(data_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0JWVsgdVKuq_"
      },
      "outputs": [],
      "source": [
        "large = len([i for i in data_train if len(i.split()) >= 20])*100 / len(data_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bEFxW9TgKyzQ"
      },
      "outputs": [],
      "source": [
        "print(f'Процент отзывов, где менее 3 слов: {small}')\n",
        "print(f'Процент отзывов, где от 3 до 19 слов: {medium}')\n",
        "print(f'Процент отзывов, где более 19 слов: {large}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTYfXO0uLo19"
      },
      "source": [
        "Количество коротких отзывов около 6%, это не значительно"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ftBTXda9NUGX"
      },
      "outputs": [],
      "source": [
        "len(data_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpdrPpjHOMDI"
      },
      "source": [
        "2. Построим WordCloud - проанализируем частотность слов"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bzyWa1AJNw7h"
      },
      "outputs": [],
      "source": [
        "!pip install wordcloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Dkh7QUROQdA"
      },
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud\n",
        "all_train_text = \" \".join(data_train)\n",
        "\n",
        "wc = WordCloud(width=500, height=300, background_color=\"white\",\n",
        "               colormap=\"viridis\", max_words=100).generate(all_train_text)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.imshow(wc, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrGvbwgTawrM"
      },
      "source": [
        "# 2. Auto-labeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VtS_o4pa3W6"
      },
      "source": [
        "Zero-shot классификация"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "Py4xvnaW7V7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# модель\n",
        "classifier = pipeline(\n",
        "    \"zero-shot-classification\",\n",
        "    model=\"MoritzLaurer/deberta-v3-large-zeroshot-v2.0\",\n",
        "    device=0 if torch.cuda.is_available() else -1\n",
        ")"
      ],
      "metadata": {
        "id": "S9CswgqixvbB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txiBihoxchDV"
      },
      "outputs": [],
      "source": [
        "category = ['бытовая техника', 'обувь', 'одежда', 'посуда', 'текстиль', 'товары для детей', 'украшения и аксессуары', 'электроника', 'нет товара']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def zero_shot_labeling(texts, labels, confidence_threshold=0.7):\n",
        "    results = []\n",
        "\n",
        "    for i, text in enumerate(texts):\n",
        "        if i % 100 == 0:\n",
        "            print(f\"Обработано {i}/{len(texts)} текстов...\")\n",
        "\n",
        "        result = classifier(text, labels, multi_label=False)\n",
        "\n",
        "        predicted_label = result['labels'][0]\n",
        "        confidence = result['scores'][0]\n",
        "\n",
        "        results.append({\n",
        "            'text': text,\n",
        "            'predicted_label': predicted_label,\n",
        "            'confidence': confidence,\n",
        "            'keep_for_training': confidence >= confidence_threshold\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(results)"
      ],
      "metadata": {
        "id": "FFbEjjvXx8Ww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# убираем пустые тексты\n",
        "data_train = [i for i in data_train if len(i) > 0]"
      ],
      "metadata": {
        "id": "ohafl6_u8Rmk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(data_train)"
      ],
      "metadata": {
        "id": "4pesymgU8lRE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "auto_labeled_df = zero_shot_labeling(data_train, category)"
      ],
      "metadata": {
        "id": "Eprd98dkx9Mb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "auto_labeled_df.head()"
      ],
      "metadata": {
        "id": "ZuHAQkBMCX6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Получили размеченный датасет с признаками confidence и keep_for_training. Для дальнейшего дообучения модели оставим только те, что keep_for_training=True"
      ],
      "metadata": {
        "id": "E9KoOWkMDT_-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qual_data_train = auto_labeled_df[auto_labeled_df['keep_for_training'] == True]"
      ],
      "metadata": {
        "id": "3dX51pfIDi0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# остаётся совсем не много качественный примеров\n",
        "qual_data_train.shape"
      ],
      "metadata": {
        "id": "cUp4fS8_Do9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# распределение уверенности\n",
        "plt.figure(figsize=(10, 6))\n",
        "auto_labeled_df['confidence'].hist(bins=20)\n",
        "plt.title('Распределение уверенности модели')\n",
        "plt.xlabel('Уверенность')\n",
        "plt.ylabel('Количество примеров')\n",
        "plt.axvline(x=0.7, color='r', linestyle='--', label='Порог 0.7')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Статистика по категориям\n",
        "category_stats = auto_labeled_df.groupby('predicted_label').agg({\n",
        "    'confidence': 'mean',\n",
        "    'keep_for_training': 'sum'\n",
        "}).sort_values('keep_for_training', ascending=False)\n",
        "\n",
        "print(\"Статистика по категориям:\")\n",
        "print(category_stats)"
      ],
      "metadata": {
        "id": "zTi_NnmKDoU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для электроники, обуви и нет товара нет (или почти нет) хороших обучающих примеров. Возьмём хотя бы несколько с самыми высокими confidence."
      ],
      "metadata": {
        "id": "3dQzouXoEY3n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rare_categories = [\"электроника\", \"обувь\", \"нет товара\"]\n",
        "\n",
        "min_examples_per_category = 10  # Минимум примеров на категорию\n",
        "\n",
        "supplemental_data = []\n",
        "\n",
        "for category in rare_categories:\n",
        "    # Берем все примеры этой категории\n",
        "    category_examples = auto_labeled_df[auto_labeled_df['predicted_label'] == category]\n",
        "\n",
        "    if len(category_examples) > 0:\n",
        "        # Сортируем по уверенности (от высокой к низкой)\n",
        "        sorted_examples = category_examples.sort_values('confidence', ascending=False)\n",
        "\n",
        "        # Берем топ-N примеров\n",
        "        n_to_take = min(min_examples_per_category, len(sorted_examples))\n",
        "        top_examples = sorted_examples.head(n_to_take)\n",
        "\n",
        "        print(f\"Категория '{category}': взято {n_to_take} лучших примеров\")\n",
        "        supplemental_data.append(top_examples)\n",
        "    else:\n",
        "        print(f\"Категория '{category}': нет примеров вообще\")\n",
        "\n",
        "# Объединяем с основными качественными данными\n",
        "if supplemental_data:\n",
        "    supplemental_df = pd.concat(supplemental_data)\n",
        "    # ПОНИЖАЕМ ПОРОГ ДЛЯ ЭТИХ КАТЕГОРИЙ - помечаем как годные для обучения\n",
        "    supplemental_df['keep_for_training'] = True\n",
        "\n",
        "    # Объединяем с основными данными\n",
        "    final_training_data = pd.concat([\n",
        "        qual_data_train,  # исходные качественные данные\n",
        "        supplemental_df     # дополнение для редких категорий\n",
        "    ])\n",
        "else:\n",
        "    final_training_data = qual_data_train"
      ],
      "metadata": {
        "id": "39akmtIDDoRH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_training_data.shape"
      ],
      "metadata": {
        "id": "aX8pKRFJFGrW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# сохраним все предсказания\n",
        "final_training_data.to_csv('final_training_data.csv', index=False)"
      ],
      "metadata": {
        "id": "gp3SxK7uFVEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_training_data.head()"
      ],
      "metadata": {
        "id": "pI1lTuShGRfj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8Oe2K7Ssgai"
      },
      "source": [
        "**Получили размеченный датасет**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6WTdefujtlOP"
      },
      "outputs": [],
      "source": [
        "X = final_training_data['text'].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "category_ind = {'бытовая техника': 0, 'обувь': 1, 'одежда': 2,\n",
        "                'посуда': 3, 'текстиль': 4, 'товары для детей': 5,\n",
        "                'украшения и аксессуары': 6, 'электроника': 7, 'нет товара': 8}"
      ],
      "metadata": {
        "id": "3K9P6DlCHKp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_labels = final_training_data['predicted_label'].tolist()"
      ],
      "metadata": {
        "id": "5GgooyAbGnBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tk0QLEf4ssyE"
      },
      "outputs": [],
      "source": [
        "y = [category_ind[label] for label in pred_labels]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(X, y, test_size=0.33, random_state=42)"
      ],
      "metadata": {
        "id": "XcVyU9XmIBl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Дообучение модели с помощью LoRA и получение предсказание на test, в том числе среднее время обработки"
      ],
      "metadata": {
        "id": "zUBRuaZMHjjJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets accelerate peft"
      ],
      "metadata": {
        "id": "Ky-_OuhTHoGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "from datasets import Dataset\n",
        "from peft import LoraConfig, get_peft_model\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "Hrqf2Lw6Hvf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# базовая модель и токенизатор\n",
        "model_name = \"cointegrated/rubert-tiny\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "base_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=9)"
      ],
      "metadata": {
        "id": "9BNFmGxqHvbT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"query\", \"key\", \"value\", \"dense\"], # адаптируем только attention-слои\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=\"SEQ_CLS\"\n",
        ")\n",
        "model = get_peft_model(base_model, lora_config)"
      ],
      "metadata": {
        "id": "8bFO9NBFHvY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enc_train = tokenizer(train_texts, truncation=True, padding=True, max_length=128)\n",
        "enc_val = tokenizer(val_texts, truncation=True, padding=True, max_length=128)"
      ],
      "metadata": {
        "id": "PTb6NOgNHvWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = Dataset.from_dict({**enc_train, \"labels\": train_labels})\n",
        "val_ds = Dataset.from_dict({**enc_val, \"labels\": val_labels})"
      ],
      "metadata": {
        "id": "6bE01hG8HvKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "    return {\"f1_weighted\": f1_score(labels, preds, average=\"weighted\")}"
      ],
      "metadata": {
        "id": "_dlyKc0CIrC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    num_train_epochs=50,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=32,\n",
        "    gradient_accumulation_steps=2,\n",
        "    learning_rate=1e-4,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=50,\n",
        "    metric_for_best_model=\"f1_weighted\",\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=val_ds,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "xAlqJL6sIxBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Метрика вышла на плато -> останавливаю обучение"
      ],
      "metadata": {
        "id": "fwm_CyRQKoA9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "best_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"best_rubert_model\",\n",
        "    num_labels=9,  # ← КРИТИЧЕСКИ ВАЖНО: укажите 9 классов!\n",
        "    ignore_mismatched_sizes=True  # игнорировать несовпадение размеров\n",
        ")"
      ],
      "metadata": {
        "id": "5eQPuOy8Klgg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Посмотрим на sample submission"
      ],
      "metadata": {
        "id": "zNAvHqt9LSWX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample = pd.read_csv('submission_example.csv')\n",
        "sample.head(3)"
      ],
      "metadata": {
        "id": "xq8NMp5CLV39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Загружаем test.csv"
      ],
      "metadata": {
        "id": "Bro88-jfLwTH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_df = pd.read_csv('test.csv')\n",
        "test_texts = test_df['text'].tolist()"
      ],
      "metadata": {
        "id": "cHjgNCNwL0CW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df.shape"
      ],
      "metadata": {
        "id": "Z6J66XvAL9Ec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time"
      ],
      "metadata": {
        "id": "9rjxb61HNyB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_categories(texts, model, tokenizer, label_map):  # с замером среднего времени обработки\n",
        "    predictions = []\n",
        "    work_time = []\n",
        "    for i, text in enumerate(texts):\n",
        "        begin_time = time.time()\n",
        "        if i % 100 == 0:\n",
        "            print(f\"Обработано {i}/{len(texts)}...\")\n",
        "\n",
        "        # Токенизация\n",
        "        inputs = tokenizer(\n",
        "            text,\n",
        "            return_tensors=\"pt\",\n",
        "            truncation=True,\n",
        "            max_length=512,\n",
        "            padding=True\n",
        "        )\n",
        "\n",
        "        # Предсказание\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            logits = outputs.logits\n",
        "            predicted_class = torch.argmax(logits, dim=-1).item()\n",
        "\n",
        "        # Преобразование в текстовую метку\n",
        "        predicted_label = label_map[predicted_class]\n",
        "        predictions.append(predicted_label)\n",
        "        end_time = time.time()\n",
        "        work_time.append(end_time - begin_time)\n",
        "\n",
        "    return predictions, work_time"
      ],
      "metadata": {
        "id": "_vZF6OsmMFCm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "category_ind  # надо сделать обратный"
      ],
      "metadata": {
        "id": "XXF6p3vtMKru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "category_ind_back = {\n",
        "    0: 'бытовая техника', 1: 'обувь', 2: 'одежда',\n",
        "    3: 'посуда', 4: 'текстиль', 5: 'товары для детей',\n",
        "    6: 'украшения и аксессуары', 7: 'электроника', 8: 'нет товара'\n",
        "}"
      ],
      "metadata": {
        "id": "nDpg-JzWMch-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_labels = predict_categories(test_texts, best_model, tokenizer, category_ind_back)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "xvfVjhPVMN8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission_df = pd.DataFrame({\n",
        "    'category': predicted_labels[0]\n",
        "})\n",
        "\n",
        "submission_df.to_csv('my_submission1.csv', index=False)"
      ],
      "metadata": {
        "id": "CQKTYGXlNLQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "time_preprocess = np.array(predicted_labels[1]).mean()"
      ],
      "metadata": {
        "id": "Lu2wYZCJOZ7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Среднее время обработки 1 текста в тестовой выборке = {time_preprocess}')"
      ],
      "metadata": {
        "id": "v13YhTLMOijj"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}